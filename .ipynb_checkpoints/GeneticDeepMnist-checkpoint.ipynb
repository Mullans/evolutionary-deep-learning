{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 326,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "np.random.seed(8675309)\n",
    "import copy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 292,
   "metadata": {},
   "outputs": [],
   "source": [
    "def show_examples(x, y):\n",
    "    plt.figure(figsize=(5,6))\n",
    "    for i in range(9):\n",
    "        plt.subplot(3, 3, i+1)\n",
    "        plt.imshow(x[i], cmap='gray', interpolation='none')\n",
    "        plt.title(\"Class {}\".format(y[i]))\n",
    "\n",
    "def norm_shift(count, high=3, stddev=1):\n",
    "    '''\n",
    "    Get a shift from a normal distribution. All shifts have an absolute value 1->high. Values are\n",
    "    chosen from a normal distribution with mean 0 and the input stddev then rounded up to the nearest\n",
    "    absolute value (ie: 0.5 -> 1 and -0.5 -> -1).\n",
    "    count -- the number of shifts to return\n",
    "    high -- clipped maximum, any value from the normal distribution with an absolute value > high\n",
    "            is clipped to +/- high\n",
    "    stddev -- the stddev of the normal distribution around 0, does not affect high\n",
    "    Returns: 1d array of size count with possible values [1, high] both inclusive\n",
    "    '''\n",
    "    x = np.random.normal(loc=0, scale=1, size=count).astype(np.float32)\n",
    "    x_new = (np.clip(np.ceil(np.absolute(x)), 1, high) * np.sign(x)).astype(np.int32)\n",
    "    return x_new"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 435,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Gene(object):\n",
    "    activations = {\n",
    "            1: 'relu',\n",
    "            2: 'selu',\n",
    "            3: 'tanh',\n",
    "            4: 'sigmoid'\n",
    "        }\n",
    "\n",
    "    def __init__(self, mutate_prob=0.0001):\n",
    "        self.num_layers = 2\n",
    "        self.fitness = -1\n",
    "        # Size of hidden layers, can be 50->1000 by 50\n",
    "        self.h = np.random.randint(low=1, high=21, size=(self.num_layers), dtype=np.int32)*50\n",
    "        # Enumerated activation functions, see get_func(a)\n",
    "        indices = np.random.randint(low=1, high=5, size=(self.num_layers))\n",
    "        self.a = [activations[i] for i in indices]\n",
    "        # Dropout rate\n",
    "        self.d = np.round(np.random.uniform(0.1, 1, size=3), 2)\n",
    "        # Batch size\n",
    "        self.b = np.random.randint(low=1, high=7, dtype=np.int32)\n",
    "        \n",
    "    def __call__(self):\n",
    "        from keras.models import Sequential\n",
    "        from keras.layers.core import Dense, Dropout\n",
    "        model = Sequential()\n",
    "        model.add(Dense(self.h[0], activation=self.a[0], input_shape=(784,), name=\"Dense1\"))\n",
    "        model.add(Dropout(self.d[0], name=\"Drop1\"))\n",
    "        for i in range(1, self.num_layers):\n",
    "            model.add(Dense(self.h[i], activation=self.a[i], input_shape=(784,), name=\"Dense\"+str(i+1)))\n",
    "            model.add(Dropout(self.d[i], name=\"Drop\"+str(i+1)))\n",
    "        model.add(Dense(10, activation='softmax', name=\"output\"))\n",
    "        return model\n",
    "    \n",
    "    def __str__(self):\n",
    "        output = []\n",
    "        output.append('_________________________________________________________________')\n",
    "        output.append('input\\t[size: {:{align}{width}}'.format(784,align='<', width='7')+\n",
    "                      'batch size: {:{align}{width}}]'.format(2**self.b, align='<', width='25'))\n",
    "        for i in range(self.num_layers):\n",
    "            output.append('layer {}\\t[size: {:{align}{width}}'.format(i+1, self.h[i],align='<', width='7')+\n",
    "                          'activation: {:{align}{width}}drop%: {:.2f}]'.format(self.a[i],self.d[i], align='<', width='14'))\n",
    "        output.append('output\\t[size: {:{align}{width}}'.format(10, align='<', width='7')+\n",
    "                      'activation: {:{align}{width}}]'.format('softmax', align='<', width='25'))\n",
    "        output.append('_________________________________________________________________')\n",
    "        return '\\n'.join(output)\n",
    "        \n",
    "    def mutate(self):\n",
    "        h_shift = norm_shift(self.num_layers)*np.random.binomial(1, self.mutate_prob, self.num_layers)*50\n",
    "        self.h = np.clip(self.h+h_shift, 50, 1000)\n",
    "        a_shift = np.random.binomial(1, self.mutate_prob, self.num_layers)\n",
    "        for i in range(self.num_layers):\n",
    "            if a_shift[i]:\n",
    "                self.a[i] = activations[np.random.randint(low=1, high=5)]\n",
    "        d_shift = np.random.normal(loc=0, scale=0.1, size=self.num_layers)*np.random.binomial(1, self.mutate_prob, self.num_layers)\n",
    "        self.d = np.clip(self.d+d_shift, 0.1, 1)\n",
    "        if np.random.binomial(1, self.mutate_prob):\n",
    "            self.b = np.clip(self.b+norm_shift(self.num_layers), 1, 6)\n",
    "            \n",
    "    def cross(self, other):\n",
    "        cross_mask = np.random.binomial(1, 0.5, (4, 3))\n",
    "        new_gene = copy.copy(self)\n",
    "        for i in range(self.num_layers):\n",
    "            if cross_mask[0,i]: new_gene.a[i] = other.a[i]\n",
    "            if cross_mask[1,i]: new_gene.d[i] = other.d[i]\n",
    "            if cross_mask[2,i]: new_gene.h[i] = other.h[i]\n",
    "        if cross_mask[3,0]: new_gene.b = other.b\n",
    "        return new_gene\n",
    "                \n",
    "    def batch_size(self):\n",
    "        return 2**self.b"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 442,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Population(object):\n",
    "\n",
    "    def __init__(self, count):\n",
    "        from keras.datasets import mnist\n",
    "        from keras.utils import np_utils\n",
    "        (x_train, y_train), (x_test, y_test) = mnist.load_data()\n",
    "        print(\"X train/test shapes: {} / {}\".format(x_train.shape, x_test.shape))\n",
    "        print(\"Y train/test shapes: {} / {}\".format(y_train.shape, y_test.shape))\n",
    "        self.x_train = x_train.reshape(x_train.shape[0], 784).astype('float32') / 255\n",
    "        self.x_test = x_test.reshape(x_test.shape[0], 784).astype('float32') / 255\n",
    "        self.y_train = np_utils.to_categorical(y_train, 10)\n",
    "        self.y_test = np_utils.to_categorical(y_test, 10)\n",
    "        \n",
    "        self.population = []\n",
    "        for i in range(count):\n",
    "            self.population.append(Gene())\n",
    "    \n",
    "    def get_fitness(self, i, epochs=10, logging=False):\n",
    "        from keras import metrics\n",
    "        from keras.callbacks import ModelCheckpoint, CSVLogger, TensorBoard\n",
    "        model = self.population[i]()\n",
    "        model.compile(loss='categorical_crossentropy', optimizer='adam', metrics=[metrics.categorical_accuracy])\n",
    "        callback_list = [\n",
    "            ModelCheckpoint(\"best_accuracy.hdf5\",\n",
    "                        monitor=\"val_categorical_accuracy\",\n",
    "                        save_best_only=True, mode='max'),\n",
    "            CSVLogger(\"training.log\"),\n",
    "            TensorBoard()\n",
    "        ] if logging else []\n",
    "        print(\"Fitting Gene Model #{}\".format(i+1))\n",
    "        history_callback = model.fit(self.x_train, self.y_train,\n",
    "                          batch_size=self.population[i].batch_size(), epochs=epochs,\n",
    "                          verbose=1, validation_split=0.1,\n",
    "                          callbacks=callback_list)\n",
    "        if logging: \n",
    "            loss_history = history_callback.history[\"loss\"]\n",
    "            np.savetxt(\"loss_history.txt\", np.array(loss_history), delimiter=\",\")\n",
    "        score = model.evaluate(self.x_test, self.y_test, verbose=0)\n",
    "        #TODO: Add punishment for time to train/model complexity\n",
    "        self.population[i].fitness = score[1] \n",
    "        \n",
    "    \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 447,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "_________________________________________________________________\n",
      "input\t[size: 784    batch size: 32                       ]\n",
      "layer 1\t[size: 800    activation: relu          drop%: 0.61]\n",
      "layer 2\t[size: 450    activation: tanh          drop%: 0.75]\n",
      "output\t[size: 10     activation: softmax                  ]\n",
      "_________________________________________________________________\n",
      "Fitting Gene Model #1\n",
      "Train on 54000 samples, validate on 6000 samples\n",
      "Epoch 1/2\n",
      "54000/54000 [==============================] - 4s 66us/step - loss: 0.3983 - categorical_accuracy: 0.8763 - val_loss: 0.1186 - val_categorical_accuracy: 0.9630\n",
      "Epoch 2/2\n",
      "54000/54000 [==============================] - 3s 61us/step - loss: 0.2355 - categorical_accuracy: 0.9305 - val_loss: 0.0963 - val_categorical_accuracy: 0.9698\n",
      "0.9667\n",
      "0.9667\n",
      "_________________________________________________________________\n",
      "input\t[size: 784    batch size: 8                        ]\n",
      "layer 1\t[size: 250    activation: relu          drop%: 0.40]\n",
      "layer 2\t[size: 150    activation: selu          drop%: 0.24]\n",
      "output\t[size: 10     activation: softmax                  ]\n",
      "_________________________________________________________________\n",
      "Fitting Gene Model #2\n",
      "Train on 54000 samples, validate on 6000 samples\n",
      "Epoch 1/2\n",
      "54000/54000 [==============================] - 12s 224us/step - loss: 0.3151 - categorical_accuracy: 0.9041 - val_loss: 0.1144 - val_categorical_accuracy: 0.9678\n",
      "Epoch 2/2\n",
      "54000/54000 [==============================] - 12s 218us/step - loss: 0.1970 - categorical_accuracy: 0.9429 - val_loss: 0.0850 - val_categorical_accuracy: 0.9747\n",
      "0.9689\n",
      "0.9689\n",
      "_________________________________________________________________\n",
      "input\t[size: 784    batch size: 2                        ]\n",
      "layer 1\t[size: 550    activation: selu          drop%: 1.00]\n",
      "layer 2\t[size: 850    activation: tanh          drop%: 0.56]\n",
      "output\t[size: 10     activation: softmax                  ]\n",
      "_________________________________________________________________\n",
      "Fitting Gene Model #3\n",
      "Train on 54000 samples, validate on 6000 samples\n",
      "Epoch 1/2\n",
      "54000/54000 [==============================] - 50s 929us/step - loss: 0.7271 - categorical_accuracy: 0.8417 - val_loss: 0.3459 - val_categorical_accuracy: 0.9275\n",
      "Epoch 2/2\n",
      "54000/54000 [==============================] - 50s 924us/step - loss: 0.6358 - categorical_accuracy: 0.8932 - val_loss: 0.2857 - val_categorical_accuracy: 0.9472\n",
      "0.9363\n",
      "0.9363\n",
      "0.9667\n",
      "0.9689\n",
      "0.9363\n"
     ]
    }
   ],
   "source": [
    "pop_size = 3\n",
    "pop = Population(3)\n",
    "for i in range(pop_size):\n",
    "    print(pop.population[i])\n",
    "    pop.get_fitness(i, epochs=2)\n",
    "for i in range(pop_size):\n",
    "    print(pop.population[i].fitness)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
