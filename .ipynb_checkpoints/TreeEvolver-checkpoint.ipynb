{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/seanmullan/Code/.virtualenvs/tensorflow/lib/python3.5/site-packages/h5py/__init__.py:36: FutureWarning: Conversion of the second argument of issubdtype from `float` to `np.floating` is deprecated. In future, it will be treated as `np.float64 == np.dtype(float).type`.\n",
      "  from ._conv import register_converters as _register_converters\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import tensorflow as tf\n",
    "from timeit import default_timer as tic\n",
    "\n",
    "tf.logging.set_verbosity(tf.logging.ERROR)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def antifix(x):\n",
    "    \"\"\"Similar to numpy's fix function, but rounds away from 0\"\"\"\n",
    "    return (np.sign(x) * np.ceil(np.absolute(x))).astype(int)\n",
    "    \n",
    "\n",
    "def shift(count=None):\n",
    "    \"\"\"Get an array of ints from a normal distribution centered around 0 with a stddev of 1\n",
    "    --returns a single int rather than an array if count is None\n",
    "    \"\"\"\n",
    "    return constrain(antifix(np.random.normal(size=count)), -3, 3)\n",
    "\n",
    "def either(p=None):\n",
    "    \"\"\"Shortcut for a boolean choice with probabilities\"\"\"\n",
    "    return np.random.choice(2, p=p)\n",
    "\n",
    "def constrain(x, low, high, decimals=2):\n",
    "    \"\"\"Constrains the value to the given range (inclusive) and rounds to number of decimals\"\"\"\n",
    "    value = None\n",
    "    if x < low:\n",
    "        value = low\n",
    "    elif x > high:\n",
    "        value = high\n",
    "    else:\n",
    "        value = x\n",
    "    return np.around(value, decimals)\n",
    "    \n",
    "def distrib(x):\n",
    "    \"\"\"Softmax, but doubles values to increase disparity\"\"\"\n",
    "    x_exp = np.exp(np.array(x) * 2)\n",
    "    return x_exp / x_exp.sum()\n",
    "\n",
    "def init_drop(p=None):\n",
    "    \"\"\"Return a random initial dropout\"\"\"\n",
    "    return np.around(np.random.uniform(0, 0.5), 2)\n",
    "\n",
    "activations = [None, tf.nn.tanh, tf.nn.relu, tf.nn.leaky_relu, tf.nn.sigmoid]\n",
    "def init_act(p=None):\n",
    "    \"\"\"Return a random initial activation function\"\"\"\n",
    "    return np.random.choice(np.arange(len(activations)), p=p)\n",
    "    \n",
    "def init_hsize():\n",
    "    \"\"\"Return a random initial fully-connected layer size\n",
    "    values are [4,10] since hidden size will be 2^x for whatever x is returned\"\"\"\n",
    "    return np.random.randint(4,10 + 1)\n",
    "\n",
    "def init_lr():\n",
    "    return constrain(np.random.exponential(scale=0.09), 0.001, 0.3, decimals=4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"V2.0 - Tree-like structure, but no branching yet\"\"\"\n",
    "class Node(object):\n",
    "    \n",
    "    def __init__(self):\n",
    "        self.child = None  # will only be 1 child for this version\n",
    "        self.depth = 0\n",
    "\n",
    "    def __iadd__(self, node):\n",
    "        if self.child:\n",
    "            node.depth += 1\n",
    "            self.child += node\n",
    "        else:\n",
    "            self.child = node\n",
    "        return self\n",
    "    \n",
    "    def get(self, depth):\n",
    "        if self.child is None:\n",
    "            return None\n",
    "        if depth==0:\n",
    "            return self.child\n",
    "        else:\n",
    "            return self.child.get(depth-1)\n",
    "        \n",
    "class DenseNode(Node):\n",
    "    \n",
    "    def __init__(self, config=None):\n",
    "        Node.__init__(self)\n",
    "        \n",
    "        if config:\n",
    "            self.dropout = config[0]\n",
    "            self.activation_func = config[1]\n",
    "            self.size = config[2]\n",
    "        else:\n",
    "            self.dropout = init_drop()\n",
    "            self.activation_func = init_act()\n",
    "            self.size = init_hsize()\n",
    "        \n",
    "        self.type = \"Dense\"\n",
    "            \n",
    "    def show(self):\n",
    "        acts = ['Linear','Tanh','ReLU','LeakyReLU','Sigmoid']\n",
    "        print(\"\\tSize: {}\\tActivation: {}\\tDropout: {}\".format(2**self.size, acts[self.activation_func], self.dropout))\n",
    "        if(self.child):\n",
    "            self.child.show()\n",
    "            \n",
    "    def __call__(self, x, mode):\n",
    "        dense = tf.layers.dense(inputs=x, units=2**self.size, activation=activations[self.activation_func])\n",
    "        if self.dropout != 0:\n",
    "                dense = tf.layers.dropout(inputs=dense, rate=self.dropout, training=mode == tf.estimator.ModeKeys.TRAIN)\n",
    "        if self.child:\n",
    "            return self.child(dense, mode)\n",
    "        return dense\n",
    "    \n",
    "    def trim(self, depth):\n",
    "        if depth==0:\n",
    "            self.child = None\n",
    "        else:\n",
    "            self.child.trim(depth-1)\n",
    "    \n",
    "    def mutate(self, mutation_rate):\n",
    "        if np.random.rand() < mutation_rate:\n",
    "            self.size = constrain(self.size + shift(), 4, 10)\n",
    "            \n",
    "        if np.random.rand() < mutation_rate:\n",
    "            self.dropout = constrain(self.dropout + np.random.normal(scale=0.05), 0, 0.5)\n",
    "            \n",
    "        if np.random.rand() < mutation_rate:\n",
    "            self.activation_func = init_act()\n",
    "            \n",
    "        if self.child:\n",
    "            self.child.mutate(mutation_rate)\n",
    "            \n",
    "    def crossover(self, other, p):\n",
    "        config = [self.dropout, self.activation_func, self.size]\n",
    "        if either(p):\n",
    "            config[0] = other.dropout\n",
    "        if either(p):\n",
    "            config[1] = other.activation_func\n",
    "        if either(p):\n",
    "            config[2] = other.size\n",
    "        return config\n",
    "    \n",
    "    def config(self):\n",
    "        return [self.dropout, self.activation_func, self.size]\n",
    "    \n",
    "    \n",
    "class Tree(object):\n",
    "    \"\"\"Acts as the root as well\"\"\"\n",
    "    def __init__(self, name, mutation_rate=0.001, grow_prob=0.1, shrink_prob=0.1, num_nodes=0, load=False):\n",
    "        self.x = tf.placeholder(tf.float32, shape=[None, 784])\n",
    "        self.child = None  # will only be 1 child for now\n",
    "        self.name = name\n",
    "        \n",
    "        self.size = 0\n",
    "        self.fitness = 0\n",
    "        self.mutation_rate = mutation_rate\n",
    "        self.grow_prob = grow_prob\n",
    "        self.shrink_prob = shrink_prob\n",
    "        self.age = 0  # Total number of epochs seen\n",
    "        self.accuracy = -1\n",
    "        self.loss = -1\n",
    "        self.train_time = -1\n",
    "        \n",
    "        self.learning_rate = init_lr()#0.001\n",
    "\n",
    "        for i in range(num_nodes):\n",
    "            self += DenseNode()\n",
    "            \n",
    "        if load:\n",
    "            self.classifier = tf.estimator.Estimator(\n",
    "                model_fn=self.model_fn,\n",
    "                model_dir=\"./\"+self.name\n",
    "            )\n",
    "        else:\n",
    "            self.classifier = tf.estimator.Estimator(\n",
    "                model_fn=self.model_fn\n",
    "            )\n",
    "            \n",
    "    def show(self):\n",
    "        print(\"---------------------------------------------\")\n",
    "        print(self.name+\"\\tlr:{}\".format(self.learning_rate))\n",
    "        self.child.show()\n",
    "        print(\"---------------------------------------------\")\n",
    "\n",
    "    def update_fitness(self):\n",
    "        self.fitness = self.accuracy\n",
    "    \n",
    "    def name(self):\n",
    "        return self.name\n",
    "    \n",
    "    def train(self, input_fn, verbose=False, steps=20000):\n",
    "        logging_hooks = None\n",
    "        if verbose:\n",
    "            logged_vars = {\"probabilities\": \"softmax_tensor\"}\n",
    "            logging_hooks = [tf.train.LoggingTensorHook(tensors=logged_vars, every_n_iter=2000)]\n",
    "\n",
    "        start = tic()\n",
    "        self.classifier.train(\n",
    "            input_fn=input_fn,\n",
    "            steps=steps,\n",
    "            hooks=logging_hooks\n",
    "        )\n",
    "        self.train_time = tic()-start\n",
    "        self.age += steps\n",
    "        print(\"Time to train model \\\"{}\\\": {:.4f}\".format(self.name, self.train_time))\n",
    "        \n",
    "    def test(self, input_fn):\n",
    "        results = self.classifier.evaluate(input_fn)\n",
    "        self.accuracy = results['accuracy']\n",
    "        self.age = results['global_step']\n",
    "        self.loss = results['loss']\n",
    "        self.update_fitness()\n",
    "        print(\"{}: Age is now {}\".format(self.name, self.age))\n",
    "        print(\"\\t\",results)\n",
    "        return results\n",
    "        \n",
    "    def model_fn(self, features, labels, mode):\n",
    "        input_layer = tf.reshape(features[\"x\"], [-1, 784])\n",
    "        \n",
    "        hidden = self.child(input_layer, mode)\n",
    "        \n",
    "        logits =tf.layers.dense(inputs=hidden, units=10)\n",
    "        \n",
    "        predictions = {\n",
    "            \"classes\": tf.argmax(input=logits, axis=1),\n",
    "            \"probabilities\": tf.nn.softmax(logits, name=\"softmax_tensor\")\n",
    "        }\n",
    "        if mode == tf.estimator.ModeKeys.PREDICT:\n",
    "            return tf.estimator.EstimatorSpec(mode=mode, predictions=predictions)\n",
    "        \n",
    "        loss = tf.losses.sparse_softmax_cross_entropy(labels=labels, logits=logits)\n",
    "        if mode == tf.estimator.ModeKeys.TRAIN:\n",
    "            optimizer = tf.train.GradientDescentOptimizer(learning_rate=self.learning_rate)\n",
    "            train_op = optimizer.minimize(\n",
    "                loss=loss,\n",
    "                global_step=tf.train.get_global_step()\n",
    "            )\n",
    "            return tf.estimator.EstimatorSpec(mode=mode, loss=loss, train_op=train_op)\n",
    "        \n",
    "        eval_metric_ops = {\n",
    "            \"accuracy\":tf.metrics.accuracy(\n",
    "                labels=labels,\n",
    "                predictions=predictions['classes']\n",
    "            )\n",
    "        }\n",
    "        return tf.estimator.EstimatorSpec(\n",
    "            mode=mode,\n",
    "            loss=loss,\n",
    "            eval_metric_ops=eval_metric_ops\n",
    "        )\n",
    "        \n",
    "    def __call__(self):\n",
    "        result = self.children[0](self.x)\n",
    "        logits = tf.layers.dense(inputs=result, units=10, name=\"logits\")\n",
    "        \n",
    "        classes = tf.argmax(input=logits, axis=1)\n",
    "        probs = tf.nn.softmax(logits, name=\"softmax_tensor\")\n",
    "        \n",
    "    def __iadd__(self, node):  # +=\n",
    "        self.size += 1\n",
    "        if self.child:\n",
    "            node.depth += 1\n",
    "            self.child += node\n",
    "        else:\n",
    "            self.child = node\n",
    "        return self\n",
    "    \n",
    "    def get_mode(self):\n",
    "        return self.mode\n",
    "    \n",
    "    def __getitem__(self, depth):\n",
    "        if self.child is None:\n",
    "            raise IndexError(\"There is no layer at depth {}.\".format(depth))\n",
    "        if depth==0:\n",
    "            return self.child\n",
    "        else:\n",
    "            result = self.child.get(depth-1)\n",
    "            if result is None:\n",
    "                raise IndexError(\"There is no layer at depth {}.\".format(depth))\n",
    "            return result\n",
    "        \n",
    "    def mutate(self):\n",
    "        if np.random.rand() < self.mutation_rate:\n",
    "            self.learning_rate = constrain(np.exp(np.log(self.learning_rate) + shift), 0.0001, 0.5, decimals=4)\n",
    "        if either(np.array([1-self.shrink_prob, self.shrink_prob])):\n",
    "            self.child.trim(self.size-1)\n",
    "        if either(np.array([1-self.grow_prob, self.grow_prob])):\n",
    "            self += DenseNode()\n",
    "        if self.child:\n",
    "            self.child.mutate(self.mutation_rate)\n",
    "            \n",
    "    def crossover(self, other, name, mode='even'):\n",
    "        \"\"\"Crossover to get offspring of two trees\n",
    "        mode -- either even or biased. \n",
    "        even to have equal chance of using either parent for each trait or \n",
    "        biased to weight the decision based on relative fitness\n",
    "        \"\"\"\n",
    "        offspring = Tree(name, self.mutation_rate)\n",
    "        if mode=='even':\n",
    "            p = None\n",
    "        else:\n",
    "            total_fit = self.fitness + other.fitness\n",
    "            p = np.array([self.fitness / total_fit, other.fitness / total_fit])\n",
    "\n",
    "        offspring.learning_rate = other.learning_rate if either(p) else self.learning_rate\n",
    "\n",
    "        for i in range(min(self.size, other.size)):\n",
    "            offspring += DenseNode(self[i].crossover(other[i], p))\n",
    "        tail = either(p)\n",
    "        if other.size > self.size and tail:\n",
    "            for i in range(self.size, other.size):\n",
    "                offspring += DenseNode(other[i].config())\n",
    "        elif self.size > other.size and not tail:\n",
    "            for i in range(other.size, self.size):\n",
    "                offspring += DenseNode(self[i].config())\n",
    "        return offspring\n",
    "        \n",
    "        \n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Population(object):\n",
    "    def __init__(self, pop_size=10, cross='even', mutation_rate=0.001):\n",
    "        self.adult = 20000\n",
    "        self.elder = 40000\n",
    "        self.era_len = 5000\n",
    "        self.elites = 1\n",
    "        self.deaths = 3\n",
    "\n",
    "        mnist = tf.contrib.learn.datasets.load_dataset(\"mnist\")\n",
    "        train_data = mnist.train.images\n",
    "        train_labels = np.asarray(mnist.train.labels, dtype=np.int32)\n",
    "        valid_data = mnist.test.images[:int(mnist.test.images.shape[0]*0.5)]\n",
    "        test_data = mnist.test.images[int(mnist.test.images.shape[0]*0.5):]\n",
    "        valid_labels = np.asarray(mnist.test.labels[:int(mnist.test.labels.shape[0]*0.5)], dtype=np.int32)\n",
    "        test_labels = np.asarray(mnist.test.labels[int(mnist.test.labels.shape[0]*0.5):], dtype=np.int32)\n",
    "\n",
    "        # Input for training\n",
    "        self.train_fn = tf.estimator.inputs.numpy_input_fn(\n",
    "            x={\"x\":train_data},\n",
    "            y=train_labels,\n",
    "            batch_size=100,\n",
    "            num_epochs=None,\n",
    "            shuffle=True\n",
    "        )\n",
    "\n",
    "        # Input to get fitness of each individual\n",
    "        self.test_fn = tf.estimator.inputs.numpy_input_fn(\n",
    "            x={\"x\":valid_data},\n",
    "            y=valid_labels,\n",
    "            num_epochs=1,\n",
    "            shuffle=False\n",
    "        )\n",
    "\n",
    "        # Input for final testing after evolution\n",
    "        self.results_fn = tf.estimator.inputs.numpy_input_fn(\n",
    "            x={\"x\":test_data},\n",
    "            y=test_labels,\n",
    "            num_epochs=1,\n",
    "            shuffle=False\n",
    "        )\n",
    "        \n",
    "        self.pop = []\n",
    "        for i in range(pop_size):\n",
    "            new_tree = Tree('g0-'+str(i), num_nodes=1)\n",
    "            new_tree.show()\n",
    "            self.pop.append(new_tree)\n",
    "\n",
    "    def evolve(self, eras):\n",
    "        history = []\n",
    "        for e in range(eras):\n",
    "            print(\"Starting Era {}...\".format(e))\n",
    "            era_hist = []\n",
    "            ofage = 0\n",
    "            for tree in self.pop:\n",
    "                if tree.age < self.elder:\n",
    "                    tree.train(self.train_fn, steps=self.era_len)\n",
    "                if tree.age >= self.adult:\n",
    "                    ofage+=1\n",
    "                    tree.test(self.test_fn)\n",
    "                    era_hist.append(tree.fitness)\n",
    "            if ofage >= self.elites+self.deaths:\n",
    "                self.pop = sorted(self.pop, key=lambda x:x.fitness, reverse=True)\n",
    "                best = [self.pop[0], self.pop[1]]\n",
    "                count = 0\n",
    "                for i in range(len(self.pop)-1, -1, -1):\n",
    "                    if self.pop[i].age >= self.adult:\n",
    "                        self.pop[i] = best[0].crossover(best[1], 'g{}-{}'.format(e, count))\n",
    "                        self.pop[i].mutate()\n",
    "                        self.pop[i].show()\n",
    "                        count+=1\n",
    "                    if count >= self.deaths:\n",
    "                        break\n",
    "            if self.pop[0].fitness > 0:\n",
    "                print(\"Best fitness for era {}: {:.4f}\".format(e, best[0].fitness))\n",
    "            history.append(era_hist)\n",
    "        self.pop[0].test(self.results_fn)\n",
    "        print(\"Final best fitness after {} eras: {}-{:.4f}\".format(eras, self.pop[0].name, self.pop[0].fitness))\n",
    "        return history"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Extracting MNIST-data/train-images-idx3-ubyte.gz\n",
      "Extracting MNIST-data/train-labels-idx1-ubyte.gz\n",
      "Extracting MNIST-data/t10k-images-idx3-ubyte.gz\n",
      "Extracting MNIST-data/t10k-labels-idx1-ubyte.gz\n",
      "---------------------------------------------\n",
      "g0-0\tlr:0.2222\n",
      "\tSize: 64\tActivation: ReLU\tDropout: 0.18\n",
      "---------------------------------------------\n",
      "---------------------------------------------\n",
      "g0-1\tlr:0.0979\n",
      "\tSize: 512\tActivation: LeakyReLU\tDropout: 0.33\n",
      "---------------------------------------------\n",
      "---------------------------------------------\n",
      "g0-2\tlr:0.0071\n",
      "\tSize: 64\tActivation: Sigmoid\tDropout: 0.04\n",
      "---------------------------------------------\n",
      "---------------------------------------------\n",
      "g0-3\tlr:0.0123\n",
      "\tSize: 64\tActivation: ReLU\tDropout: 0.23\n",
      "---------------------------------------------\n",
      "---------------------------------------------\n",
      "g0-4\tlr:0.1289\n",
      "\tSize: 256\tActivation: ReLU\tDropout: 0.46\n",
      "---------------------------------------------\n",
      "---------------------------------------------\n",
      "g0-5\tlr:0.085\n",
      "\tSize: 256\tActivation: Linear\tDropout: 0.07\n",
      "---------------------------------------------\n",
      "---------------------------------------------\n",
      "g0-6\tlr:0.074\n",
      "\tSize: 32\tActivation: ReLU\tDropout: 0.21\n",
      "---------------------------------------------\n",
      "---------------------------------------------\n",
      "g0-7\tlr:0.1843\n",
      "\tSize: 64\tActivation: Tanh\tDropout: 0.19\n",
      "---------------------------------------------\n",
      "---------------------------------------------\n",
      "g0-8\tlr:0.1904\n",
      "\tSize: 512\tActivation: Tanh\tDropout: 0.18\n",
      "---------------------------------------------\n",
      "---------------------------------------------\n",
      "g0-9\tlr:0.0403\n",
      "\tSize: 256\tActivation: ReLU\tDropout: 0.31\n",
      "---------------------------------------------\n",
      "Starting Era 0...\n",
      "\tTraining g0-0...\n",
      "Time to train model \"g0-0\": 4.3334\n",
      "g0-0: age=5000, fitness=0\n",
      "\tTraining g0-1...\n",
      "Time to train model \"g0-1\": 8.3633\n",
      "g0-1: age=5000, fitness=0\n",
      "\tTraining g0-2...\n",
      "Time to train model \"g0-2\": 4.2879\n",
      "g0-2: age=5000, fitness=0\n",
      "\tTraining g0-3...\n",
      "Time to train model \"g0-3\": 4.2812\n",
      "g0-3: age=5000, fitness=0\n",
      "\tTraining g0-4...\n",
      "Time to train model \"g0-4\": 5.5320\n",
      "g0-4: age=5000, fitness=0\n",
      "\tTraining g0-5...\n",
      "Time to train model \"g0-5\": 5.3703\n",
      "g0-5: age=5000, fitness=0\n",
      "\tTraining g0-6...\n",
      "Time to train model \"g0-6\": 3.8174\n",
      "g0-6: age=5000, fitness=0\n",
      "\tTraining g0-7...\n",
      "Time to train model \"g0-7\": 4.2530\n",
      "g0-7: age=5000, fitness=0\n",
      "\tTraining g0-8...\n",
      "Time to train model \"g0-8\": 7.9226\n",
      "g0-8: age=5000, fitness=0\n",
      "\tTraining g0-9...\n",
      "Time to train model \"g0-9\": 5.5194\n",
      "g0-9: age=5000, fitness=0\n",
      "Starting Era 1...\n",
      "\tTraining g0-0...\n",
      "Time to train model \"g0-0\": 4.2679\n",
      "g0-0: age=10000, fitness=0\n",
      "\tTraining g0-1...\n",
      "Time to train model \"g0-1\": 8.3879\n",
      "g0-1: age=10000, fitness=0\n",
      "\tTraining g0-2...\n",
      "Time to train model \"g0-2\": 4.2280\n",
      "g0-2: age=10000, fitness=0\n",
      "\tTraining g0-3...\n",
      "Time to train model \"g0-3\": 4.2592\n",
      "g0-3: age=10000, fitness=0\n",
      "\tTraining g0-4...\n",
      "Time to train model \"g0-4\": 5.5775\n",
      "g0-4: age=10000, fitness=0\n",
      "\tTraining g0-5...\n",
      "Time to train model \"g0-5\": 5.2828\n",
      "g0-5: age=10000, fitness=0\n",
      "\tTraining g0-6...\n",
      "Time to train model \"g0-6\": 3.7785\n",
      "g0-6: age=10000, fitness=0\n",
      "\tTraining g0-7...\n",
      "Time to train model \"g0-7\": 4.2922\n",
      "g0-7: age=10000, fitness=0\n",
      "\tTraining g0-8...\n",
      "Time to train model \"g0-8\": 7.8519\n",
      "g0-8: age=10000, fitness=0\n",
      "\tTraining g0-9...\n",
      "Time to train model \"g0-9\": 5.5403\n",
      "g0-9: age=10000, fitness=0\n",
      "Starting Era 2...\n",
      "\tTraining g0-0...\n",
      "Time to train model \"g0-0\": 4.2746\n",
      "g0-0: age=15000, fitness=0\n",
      "\tTraining g0-1...\n",
      "Time to train model \"g0-1\": 8.3889\n",
      "g0-1: age=15000, fitness=0\n",
      "\tTraining g0-2...\n",
      "Time to train model \"g0-2\": 4.2640\n",
      "g0-2: age=15000, fitness=0\n",
      "\tTraining g0-3...\n",
      "Time to train model \"g0-3\": 4.2709\n",
      "g0-3: age=15000, fitness=0\n",
      "\tTraining g0-4...\n",
      "Time to train model \"g0-4\": 5.6020\n",
      "g0-4: age=15000, fitness=0\n",
      "\tTraining g0-5...\n",
      "Time to train model \"g0-5\": 5.2954\n",
      "g0-5: age=15000, fitness=0\n",
      "\tTraining g0-6...\n",
      "Time to train model \"g0-6\": 3.8042\n",
      "g0-6: age=15000, fitness=0\n",
      "\tTraining g0-7...\n",
      "Time to train model \"g0-7\": 4.2801\n",
      "g0-7: age=15000, fitness=0\n",
      "\tTraining g0-8...\n",
      "Time to train model \"g0-8\": 7.8346\n",
      "g0-8: age=15000, fitness=0\n",
      "\tTraining g0-9...\n",
      "Time to train model \"g0-9\": 5.5142\n",
      "g0-9: age=15000, fitness=0\n",
      "Starting Era 3...\n",
      "\tTraining g0-0...\n",
      "Time to train model \"g0-0\": 4.3626\n",
      "\tTesting g0-0...\n",
      "g0-0: Age is now 20000\n",
      "\t {'accuracy': 0.9668, 'global_step': 20000, 'loss': 0.1117548}\n",
      "g0-0: age=20000, fitness=0.9667999744415283\n",
      "\tTraining g0-1...\n",
      "Time to train model \"g0-1\": 8.3253\n",
      "\tTesting g0-1...\n",
      "g0-1: Age is now 20000\n",
      "\t {'accuracy': 0.9704, 'global_step': 20000, 'loss': 0.09094147}\n",
      "g0-1: age=20000, fitness=0.9703999757766724\n",
      "\tTraining g0-2...\n",
      "Time to train model \"g0-2\": 4.3106\n",
      "\tTesting g0-2...\n",
      "g0-2: Age is now 20000\n",
      "\t {'accuracy': 0.879, 'global_step': 20000, 'loss': 0.4189722}\n",
      "g0-2: age=20000, fitness=0.8790000081062317\n",
      "\tTraining g0-3...\n",
      "Time to train model \"g0-3\": 4.2504\n",
      "\tTesting g0-3...\n",
      "g0-3: Age is now 20000\n",
      "\t {'accuracy': 0.9386, 'global_step': 20000, 'loss': 0.19951077}\n",
      "g0-3: age=20000, fitness=0.9386000037193298\n",
      "\tTraining g0-4...\n",
      "Time to train model \"g0-4\": 5.5499\n",
      "\tTesting g0-4...\n",
      "g0-4: Age is now 20000\n",
      "\t {'accuracy': 0.975, 'global_step': 20000, 'loss': 0.082572594}\n",
      "g0-4: age=20000, fitness=0.9750000238418579\n",
      "\tTraining g0-5...\n",
      "Time to train model \"g0-5\": 5.2943\n",
      "\tTesting g0-5...\n",
      "g0-5: Age is now 20000\n",
      "\t {'accuracy': 0.9038, 'global_step': 20000, 'loss': 0.33801273}\n",
      "g0-5: age=20000, fitness=0.9038000106811523\n",
      "\tTraining g0-6...\n",
      "Time to train model \"g0-6\": 3.7949\n",
      "\tTesting g0-6...\n",
      "g0-6: Age is now 20000\n",
      "\t {'accuracy': 0.947, 'global_step': 20000, 'loss': 0.17227343}\n",
      "g0-6: age=20000, fitness=0.9470000267028809\n",
      "\tTraining g0-7...\n",
      "Time to train model \"g0-7\": 4.3052\n",
      "\tTesting g0-7...\n",
      "g0-7: Age is now 20000\n",
      "\t {'accuracy': 0.9628, 'global_step': 20000, 'loss': 0.1224578}\n",
      "g0-7: age=20000, fitness=0.9628000259399414\n",
      "\tTraining g0-8...\n",
      "Time to train model \"g0-8\": 7.9100\n",
      "\tTesting g0-8...\n",
      "g0-8: Age is now 20000\n",
      "\t {'accuracy': 0.973, 'global_step': 20000, 'loss': 0.08639004}\n",
      "g0-8: age=20000, fitness=0.9729999899864197\n",
      "\tTraining g0-9...\n",
      "Time to train model \"g0-9\": 5.6167\n",
      "\tTesting g0-9...\n",
      "g0-9: Age is now 20000\n",
      "\t {'accuracy': 0.9672, 'global_step': 20000, 'loss': 0.101140104}\n",
      "g0-9: age=20000, fitness=0.967199981212616\n",
      "---------------------------------------------\n",
      "g3-0\tlr:0.1289\n",
      "\tSize: 512\tActivation: ReLU\tDropout: 0.46\n",
      "---------------------------------------------\n"
     ]
    },
    {
     "ename": "NameError",
     "evalue": "name 'deaths' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-5-a393d2a6dd8a>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0mpool\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mPopulation\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 2\u001b[0;31m \u001b[0mhist\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mpool\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mevolve\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m20\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m<ipython-input-4-4a8128f8ca11>\u001b[0m in \u001b[0;36mevolve\u001b[0;34m(self, eras)\u001b[0m\n\u001b[1;32m     72\u001b[0m                         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpop\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshow\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     73\u001b[0m                         \u001b[0mcount\u001b[0m\u001b[0;34m+=\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 74\u001b[0;31m                     \u001b[0;32mif\u001b[0m \u001b[0mcount\u001b[0m \u001b[0;34m>=\u001b[0m \u001b[0mdeaths\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     75\u001b[0m                         \u001b[0;32mbreak\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     76\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpop\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfitness\u001b[0m \u001b[0;34m>\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mNameError\u001b[0m: name 'deaths' is not defined"
     ]
    }
   ],
   "source": [
    "pool = Population()\n",
    "hist = pool.evolve(20)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "tensorflow",
   "language": "python",
   "name": "tensorflow"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
