{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/seanmullan/Code/.virtualenvs/tensorflow/lib/python3.5/site-packages/h5py/__init__.py:36: FutureWarning: Conversion of the second argument of issubdtype from `float` to `np.floating` is deprecated. In future, it will be treated as `np.float64 == np.dtype(float).type`.\n",
      "  from ._conv import register_converters as _register_converters\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import tensorflow as tf\n",
    "from timeit import default_timer as tic\n",
    "\n",
    "tf.logging.set_verbosity(tf.logging.ERROR)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def antifix(x):\n",
    "    \"\"\"Similar to numpy's fix function, but rounds away from 0\"\"\"\n",
    "    return (np.sign(x) * np.ceil(np.absolute(x))).astype(int)\n",
    "    \n",
    "\n",
    "def shift(count=None):\n",
    "    \"\"\"Get an array of ints from a normal distribution centered around 0 with a stddev of 1\n",
    "    --returns a single int rather than an array if count is None\n",
    "    \"\"\"\n",
    "    return constrain(antifix(np.random.normal(size=count)), -3, 3)\n",
    "\n",
    "def either(p=None):\n",
    "    \"\"\"Shortcut for a boolean choice with probabilities\"\"\"\n",
    "    return np.random.choice(2, p=p)\n",
    "\n",
    "def constrain(x, low, high, decimals=2):\n",
    "    \"\"\"Constrains the value to the given range (inclusive) and rounds to number of decimals\"\"\"\n",
    "    value = None\n",
    "    if x < low:\n",
    "        value = low\n",
    "    elif x > high:\n",
    "        value = high\n",
    "    else:\n",
    "        value = x\n",
    "    return np.around(value, decimals)\n",
    "    \n",
    "def distrib(x):\n",
    "    \"\"\"Softmax, but doubles values to increase disparity\"\"\"\n",
    "    x_exp = np.exp(np.array(x) * 2)\n",
    "    return x_exp / x_exp.sum()\n",
    "\n",
    "def init_drop(p=None):\n",
    "    \"\"\"Return a random initial dropout\"\"\"\n",
    "    return np.around(np.random.uniform(0, 0.5), 2)\n",
    "\n",
    "activations = [None, tf.nn.tanh, tf.nn.relu, tf.nn.leaky_relu, tf.nn.sigmoid]\n",
    "def init_act(p=None):\n",
    "    \"\"\"Return a random initial activation function\"\"\"\n",
    "    return np.random.choice(np.arange(len(activations)), p=p)\n",
    "    \n",
    "def init_hsize():\n",
    "    \"\"\"Return a random initial fully-connected layer size\n",
    "    values are [4,10] since hidden size will be 2^x for whatever x is returned\"\"\"\n",
    "    return np.random.randint(4,10 + 1)\n",
    "\n",
    "def init_lr():\n",
    "    return constrain(np.random.exponential(scale=0.09), 0.001, 0.3, decimals=4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"V2.0 - Tree-like structure, but no branching yet\"\"\"\n",
    "class Node(object):\n",
    "    \n",
    "    def __init__(self):\n",
    "        self.child = None  # will only be 1 child for this version\n",
    "        self.depth = 0\n",
    "\n",
    "    def __iadd__(self, node):\n",
    "        if self.child:\n",
    "            node.depth += 1\n",
    "            self.child += node\n",
    "        else:\n",
    "            self.child = node\n",
    "        return self\n",
    "    \n",
    "    def get(self, depth):\n",
    "        if self.child is None:\n",
    "            return None\n",
    "        if depth==0:\n",
    "            return self.child\n",
    "        else:\n",
    "            return self.child.get(depth-1)\n",
    "        \n",
    "class DenseNode(Node):\n",
    "    \n",
    "    def __init__(self, config=None):\n",
    "        Node.__init__(self)\n",
    "        \n",
    "        if config:\n",
    "            self.dropout = config[0]\n",
    "            self.activation_func = config[1]\n",
    "            self.size = config[2]\n",
    "        else:\n",
    "            self.dropout = init_drop()\n",
    "            self.activation_func = init_act()\n",
    "            self.size = init_hsize()\n",
    "        \n",
    "        self.type = \"Dense\"\n",
    "            \n",
    "    def show(self):\n",
    "        acts = ['Linear','Tanh','ReLU','LeakyReLU','Sigmoid']\n",
    "        print(\"\\tSize: {}\\tActivation: {}\\tDropout: {}\".format(2**self.size, acts[self.activation_func], self.dropout))\n",
    "        if(self.child):\n",
    "            self.child.show()\n",
    "            \n",
    "    def __call__(self, x, mode):\n",
    "        dense = tf.layers.dense(inputs=x, units=2**self.size, activation=activations[self.activation_func])\n",
    "        if self.dropout != 0:\n",
    "                dense = tf.layers.dropout(inputs=dense, rate=self.dropout, training=mode == tf.estimator.ModeKeys.TRAIN)\n",
    "        if self.child:\n",
    "            return self.child(dense, mode)\n",
    "        return dense\n",
    "    \n",
    "    def trim(self, depth):\n",
    "        if depth==0:\n",
    "            self.child = None\n",
    "        else:\n",
    "            self.child.trim(depth-1)\n",
    "    \n",
    "    def mutate(self, mutation_rate):\n",
    "        if np.random.rand() < mutation_rate:\n",
    "            self.size = constrain(self.size + shift(), 4, 10)\n",
    "            \n",
    "        if np.random.rand() < mutation_rate:\n",
    "            self.dropout = constrain(self.dropout + np.random.normal(scale=0.05), 0, 0.5)\n",
    "            \n",
    "        if np.random.rand() < mutation_rate:\n",
    "            self.activation_func = init_act()\n",
    "            \n",
    "        if self.child:\n",
    "            self.child.mutate(mutation_rate)\n",
    "            \n",
    "    def crossover(self, other, p):\n",
    "        config = [self.dropout, self.activation_func, self.size]\n",
    "        if either(p):\n",
    "            config[0] = other.dropout\n",
    "        if either(p):\n",
    "            config[1] = other.activation_func\n",
    "        if either(p):\n",
    "            config[2] = other.size\n",
    "        return config\n",
    "    \n",
    "    def config(self):\n",
    "        return [self.dropout, self.activation_func, self.size]\n",
    "    \n",
    "    \n",
    "class Tree(object):\n",
    "    \"\"\"Acts as the root as well\"\"\"\n",
    "    def __init__(self, name, mutation_rate=0.1, grow_prob=0.3, shrink_prob=0.15, num_nodes=0, load=False):\n",
    "        self.x = tf.placeholder(tf.float32, shape=[None, 784])\n",
    "        self.child = None  # will only be 1 child for now\n",
    "        self.name = name\n",
    "        \n",
    "        self.size = 0\n",
    "        self.fitness = 0\n",
    "        self.mutation_rate = mutation_rate\n",
    "        self.grow_prob = grow_prob\n",
    "        self.shrink_prob = shrink_prob\n",
    "        self.age = 0  # Total number of epochs seen\n",
    "        self.accuracy = -1\n",
    "        self.loss = -1\n",
    "        self.train_time = -1\n",
    "        \n",
    "        self.learning_rate = init_lr()#0.001\n",
    "\n",
    "        for i in range(num_nodes):\n",
    "            self += DenseNode()\n",
    "            \n",
    "        if load:\n",
    "            self.classifier = tf.estimator.Estimator(\n",
    "                model_fn=self.model_fn,\n",
    "                model_dir=\"./\"+self.name\n",
    "            )\n",
    "        else:\n",
    "            self.classifier = tf.estimator.Estimator(\n",
    "                model_fn=self.model_fn\n",
    "            )\n",
    "            \n",
    "    def show(self):\n",
    "        print(\"---------------------------------------------\")\n",
    "        print(self.name+\"\\tlr:{}\".format(self.learning_rate))\n",
    "        if(self.fitness!=0):\n",
    "            print(\"\\tFitness: {}\\tAccuracy: {}\\tLoss: {}\".format(self.fitness, self.accuracy, self.loss))\n",
    "        self.child.show()\n",
    "        print(\"---------------------------------------------\")\n",
    "\n",
    "    def update_fitness(self):\n",
    "        self.fitness = self.accuracy\n",
    "    \n",
    "    def name(self):\n",
    "        return self.name\n",
    "    \n",
    "    def train(self, input_fn, verbose=False, steps=20000):\n",
    "        logging_hooks = None\n",
    "        if verbose:\n",
    "            logged_vars = {\"probabilities\": \"softmax_tensor\"}\n",
    "            logging_hooks = [tf.train.LoggingTensorHook(tensors=logged_vars, every_n_iter=2000)]\n",
    "\n",
    "        start = tic()\n",
    "        self.classifier.train(\n",
    "            input_fn=input_fn,\n",
    "            steps=steps,\n",
    "            hooks=logging_hooks\n",
    "        )\n",
    "        self.train_time = tic()-start\n",
    "        self.age += steps\n",
    "        print(\"Time to train model \\\"{}\\\": {:.4f}\".format(self.name, self.train_time))\n",
    "        \n",
    "    def test(self, input_fn):\n",
    "        results = self.classifier.evaluate(input_fn)\n",
    "        self.accuracy = results['accuracy']\n",
    "        self.age = results['global_step']\n",
    "        self.loss = results['loss']\n",
    "        self.update_fitness()\n",
    "        return results\n",
    "        \n",
    "    def model_fn(self, features, labels, mode):\n",
    "        input_layer = tf.reshape(features[\"x\"], [-1, 784])\n",
    "        \n",
    "        hidden = self.child(input_layer, mode)\n",
    "        \n",
    "        logits =tf.layers.dense(inputs=hidden, units=10)\n",
    "        \n",
    "        predictions = {\n",
    "            \"classes\": tf.argmax(input=logits, axis=1),\n",
    "            \"probabilities\": tf.nn.softmax(logits, name=\"softmax_tensor\")\n",
    "        }\n",
    "        if mode == tf.estimator.ModeKeys.PREDICT:\n",
    "            return tf.estimator.EstimatorSpec(mode=mode, predictions=predictions)\n",
    "        \n",
    "        loss = tf.losses.sparse_softmax_cross_entropy(labels=labels, logits=logits)\n",
    "        if mode == tf.estimator.ModeKeys.TRAIN:\n",
    "            optimizer = tf.train.GradientDescentOptimizer(learning_rate=self.learning_rate)\n",
    "            train_op = optimizer.minimize(\n",
    "                loss=loss,\n",
    "                global_step=tf.train.get_global_step()\n",
    "            )\n",
    "            return tf.estimator.EstimatorSpec(mode=mode, loss=loss, train_op=train_op)\n",
    "        \n",
    "        eval_metric_ops = {\n",
    "            \"accuracy\":tf.metrics.accuracy(\n",
    "                labels=labels,\n",
    "                predictions=predictions['classes']\n",
    "            )\n",
    "        }\n",
    "        return tf.estimator.EstimatorSpec(\n",
    "            mode=mode,\n",
    "            loss=loss,\n",
    "            eval_metric_ops=eval_metric_ops\n",
    "        )\n",
    "        \n",
    "    def __call__(self):\n",
    "        result = self.children[0](self.x)\n",
    "        logits = tf.layers.dense(inputs=result, units=10, name=\"logits\")\n",
    "        \n",
    "        classes = tf.argmax(input=logits, axis=1)\n",
    "        probs = tf.nn.softmax(logits, name=\"softmax_tensor\")\n",
    "        \n",
    "    def __iadd__(self, node):  # +=\n",
    "        self.size += 1\n",
    "        if self.child:\n",
    "            node.depth += 1\n",
    "            self.child += node\n",
    "        else:\n",
    "            self.child = node\n",
    "        return self\n",
    "    \n",
    "    def get_mode(self):\n",
    "        return self.mode\n",
    "    \n",
    "    def __getitem__(self, depth):\n",
    "        if self.child is None:\n",
    "            raise IndexError(\"There is no layer at depth {}.\".format(depth))\n",
    "        if depth==0:\n",
    "            return self.child\n",
    "        else:\n",
    "            result = self.child.get(depth-1)\n",
    "            if result is None:\n",
    "                raise IndexError(\"There is no layer at depth {}.\".format(depth))\n",
    "            return result\n",
    "        \n",
    "    def mutate(self):\n",
    "        if np.random.rand() < self.mutation_rate:\n",
    "            self.learning_rate = constrain(np.exp(np.log(self.learning_rate) + shift()), 0.0001, 0.5, decimals=4)\n",
    "        if either(np.array([1-self.shrink_prob, self.shrink_prob])):\n",
    "            self.child.trim(self.size-1)\n",
    "        if either(np.array([1-self.grow_prob, self.grow_prob])):\n",
    "            self += DenseNode()\n",
    "        if self.child:\n",
    "            self.child.mutate(self.mutation_rate)\n",
    "            \n",
    "    def crossover(self, other, name, mode='even'):\n",
    "        \"\"\"Crossover to get offspring of two trees\n",
    "        mode -- either even or biased. \n",
    "        even to have equal chance of using either parent for each trait or \n",
    "        biased to weight the decision based on relative fitness\n",
    "        \"\"\"\n",
    "        offspring = Tree(name, self.mutation_rate)\n",
    "        if mode=='even':\n",
    "            p = None\n",
    "        else:\n",
    "            total_fit = self.fitness + other.fitness\n",
    "            p = np.array([self.fitness / total_fit, other.fitness / total_fit])\n",
    "\n",
    "        offspring.learning_rate = other.learning_rate if either(p) else self.learning_rate\n",
    "\n",
    "        for i in range(min(self.size, other.size)):\n",
    "            offspring += DenseNode(self[i].crossover(other[i], p))\n",
    "        tail = either(p)\n",
    "        if other.size > self.size and tail:\n",
    "            for i in range(self.size, other.size):\n",
    "                offspring += DenseNode(other[i].config())\n",
    "        elif self.size > other.size and not tail:\n",
    "            for i in range(other.size, self.size):\n",
    "                offspring += DenseNode(self[i].config())\n",
    "        return offspring\n",
    "        \n",
    "        \n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Population(object):\n",
    "    def __init__(self, pop_size=10, cross='even', mutation_rate=0.001):\n",
    "        self.adult = 20000\n",
    "        self.elder = 40000\n",
    "        self.era_len = 5000\n",
    "        self.elites = 1\n",
    "        self.deaths = 3\n",
    "\n",
    "        mnist = tf.contrib.learn.datasets.load_dataset(\"mnist\")\n",
    "        train_data = mnist.train.images\n",
    "        train_labels = np.asarray(mnist.train.labels, dtype=np.int32)\n",
    "        valid_data = mnist.test.images[:int(mnist.test.images.shape[0]*0.5)]\n",
    "        test_data = mnist.test.images[int(mnist.test.images.shape[0]*0.5):]\n",
    "        valid_labels = np.asarray(mnist.test.labels[:int(mnist.test.labels.shape[0]*0.5)], dtype=np.int32)\n",
    "        test_labels = np.asarray(mnist.test.labels[int(mnist.test.labels.shape[0]*0.5):], dtype=np.int32)\n",
    "\n",
    "        # Input for training\n",
    "        self.train_fn = tf.estimator.inputs.numpy_input_fn(\n",
    "            x={\"x\":train_data},\n",
    "            y=train_labels,\n",
    "            batch_size=100,\n",
    "            num_epochs=None,\n",
    "            shuffle=True\n",
    "        )\n",
    "\n",
    "        # Input to get fitness of each individual\n",
    "        self.test_fn = tf.estimator.inputs.numpy_input_fn(\n",
    "            x={\"x\":valid_data},\n",
    "            y=valid_labels,\n",
    "            num_epochs=1,\n",
    "            shuffle=False\n",
    "        )\n",
    "\n",
    "        # Input for final testing after evolution\n",
    "        self.results_fn = tf.estimator.inputs.numpy_input_fn(\n",
    "            x={\"x\":test_data},\n",
    "            y=test_labels,\n",
    "            num_epochs=1,\n",
    "            shuffle=False\n",
    "        )\n",
    "        \n",
    "        self.pop = []\n",
    "        for i in range(pop_size):\n",
    "            new_tree = Tree('g0-'+str(i), num_nodes=1)\n",
    "            new_tree.show()\n",
    "            self.pop.append(new_tree)\n",
    "\n",
    "    def evolve(self, eras):\n",
    "        history = []\n",
    "        start = tic()\n",
    "        for e in range(eras):\n",
    "            print(\"Starting Era {}...\".format(e))\n",
    "            era_hist = []\n",
    "            ofage = 0\n",
    "            for tree in self.pop:\n",
    "                if tree.age < self.elder:\n",
    "                    tree.train(self.train_fn, steps=self.era_len)\n",
    "                if tree.age >= self.adult:\n",
    "                    ofage+=1\n",
    "                    tree.test(self.test_fn)\n",
    "                    era_hist.append(tree.fitness)\n",
    "            if ofage >= self.elites+self.deaths:\n",
    "                self.pop = sorted(self.pop, key=lambda x:x.fitness, reverse=True)\n",
    "                best = [self.pop[0], self.pop[1]]\n",
    "                count = 0\n",
    "                for i in range(len(self.pop)-1, -1, -1):\n",
    "                    if self.pop[i].age >= self.adult:\n",
    "                        self.pop[i] = best[0].crossover(best[1], 'g{}-{}'.format(e, count))\n",
    "                        self.pop[i].mutate()\n",
    "                        self.pop[i].show()\n",
    "                        count+=1\n",
    "                    if count >= self.deaths:\n",
    "                        break\n",
    "            if self.pop[0].fitness > 0:\n",
    "                print(\"Best fitness for era {}: {:.4f}\".format(e, best[0].fitness))\n",
    "            history.append(era_hist)\n",
    "        self.pop[0].test(self.results_fn)\n",
    "        print(\"Final best fitness after {} eras: {} = {:.4f}\".format(eras, self.pop[0].name, self.pop[0].fitness))\n",
    "        print(\"Total training time: {:.4f}\".format(tic()-start))\n",
    "        for i in range(len(self.pop)):\n",
    "            print(\"Tree #{}\".format(i))\n",
    "            self.pop[i].show()\n",
    "        return history"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Extracting MNIST-data/train-images-idx3-ubyte.gz\n",
      "Extracting MNIST-data/train-labels-idx1-ubyte.gz\n",
      "Extracting MNIST-data/t10k-images-idx3-ubyte.gz\n",
      "Extracting MNIST-data/t10k-labels-idx1-ubyte.gz\n",
      "---------------------------------------------\n",
      "g0-0\tlr:0.0638\n",
      "\tSize: 16\tActivation: Tanh\tDropout: 0.04\n",
      "---------------------------------------------\n",
      "---------------------------------------------\n",
      "g0-1\tlr:0.2652\n",
      "\tSize: 256\tActivation: Sigmoid\tDropout: 0.13\n",
      "---------------------------------------------\n",
      "---------------------------------------------\n",
      "g0-2\tlr:0.0418\n",
      "\tSize: 32\tActivation: Tanh\tDropout: 0.08\n",
      "---------------------------------------------\n",
      "---------------------------------------------\n",
      "g0-3\tlr:0.0104\n",
      "\tSize: 16\tActivation: Linear\tDropout: 0.01\n",
      "---------------------------------------------\n",
      "---------------------------------------------\n",
      "g0-4\tlr:0.2952\n",
      "\tSize: 32\tActivation: Tanh\tDropout: 0.43\n",
      "---------------------------------------------\n",
      "---------------------------------------------\n",
      "g0-5\tlr:0.1026\n",
      "\tSize: 128\tActivation: Tanh\tDropout: 0.35\n",
      "---------------------------------------------\n",
      "---------------------------------------------\n",
      "g0-6\tlr:0.0367\n",
      "\tSize: 512\tActivation: LeakyReLU\tDropout: 0.22\n",
      "---------------------------------------------\n",
      "---------------------------------------------\n",
      "g0-7\tlr:0.0734\n",
      "\tSize: 512\tActivation: LeakyReLU\tDropout: 0.03\n",
      "---------------------------------------------\n",
      "---------------------------------------------\n",
      "g0-8\tlr:0.0287\n",
      "\tSize: 64\tActivation: Tanh\tDropout: 0.32\n",
      "---------------------------------------------\n",
      "---------------------------------------------\n",
      "g0-9\tlr:0.0121\n",
      "\tSize: 128\tActivation: Sigmoid\tDropout: 0.1\n",
      "---------------------------------------------\n",
      "Starting Era 0...\n",
      "Time to train model \"g0-0\": 3.6176\n",
      "Time to train model \"g0-1\": 5.5390\n",
      "Time to train model \"g0-2\": 3.8057\n",
      "Time to train model \"g0-3\": 3.7089\n",
      "Time to train model \"g0-4\": 3.8650\n",
      "Time to train model \"g0-5\": 4.4755\n",
      "Time to train model \"g0-6\": 8.5891\n",
      "Time to train model \"g0-7\": 8.4614\n",
      "Time to train model \"g0-8\": 4.3063\n",
      "Time to train model \"g0-9\": 4.5780\n",
      "Starting Era 1...\n",
      "Time to train model \"g0-0\": 3.6295\n",
      "Time to train model \"g0-1\": 5.5352\n",
      "Time to train model \"g0-2\": 3.9179\n",
      "Time to train model \"g0-3\": 3.6576\n",
      "Time to train model \"g0-4\": 3.8221\n",
      "Time to train model \"g0-5\": 4.5558\n",
      "Time to train model \"g0-6\": 8.4916\n",
      "Time to train model \"g0-7\": 8.4439\n",
      "Time to train model \"g0-8\": 4.3834\n",
      "Time to train model \"g0-9\": 4.4723\n",
      "Starting Era 2...\n",
      "Time to train model \"g0-0\": 3.6655\n",
      "Time to train model \"g0-1\": 5.6246\n",
      "Time to train model \"g0-2\": 3.8674\n",
      "Time to train model \"g0-3\": 3.6091\n",
      "Time to train model \"g0-4\": 3.9374\n",
      "Time to train model \"g0-5\": 4.4618\n",
      "Time to train model \"g0-6\": 8.4371\n",
      "Time to train model \"g0-7\": 8.4382\n",
      "Time to train model \"g0-8\": 4.4054\n",
      "Time to train model \"g0-9\": 4.5174\n",
      "Starting Era 3...\n",
      "Time to train model \"g0-0\": 3.6180\n",
      "Time to train model \"g0-1\": 5.5394\n",
      "Time to train model \"g0-2\": 3.8255\n",
      "Time to train model \"g0-3\": 3.6479\n",
      "Time to train model \"g0-4\": 3.8894\n",
      "Time to train model \"g0-5\": 4.4893\n",
      "Time to train model \"g0-6\": 8.5531\n",
      "Time to train model \"g0-7\": 8.4197\n",
      "Time to train model \"g0-8\": 4.3807\n",
      "Time to train model \"g0-9\": 4.5032\n",
      "---------------------------------------------\n",
      "g3-0\tlr:0.0734\n",
      "\tSize: 256\tActivation: LeakyReLU\tDropout: 0.03\n",
      "---------------------------------------------\n",
      "---------------------------------------------\n",
      "g3-1\tlr:0.2652\n",
      "\tSize: 512\tActivation: Linear\tDropout: 0.13\n",
      "---------------------------------------------\n",
      "---------------------------------------------\n",
      "g3-2\tlr:0.2652\n",
      "\tSize: 512\tActivation: Sigmoid\tDropout: 0.13\n",
      "---------------------------------------------\n",
      "Best fitness for era 3: 0.9710\n",
      "Starting Era 4...\n",
      "Time to train model \"g0-7\": 8.5531\n",
      "Time to train model \"g0-1\": 5.5488\n",
      "Time to train model \"g0-6\": 8.4947\n",
      "Time to train model \"g0-5\": 4.4271\n",
      "Time to train model \"g0-2\": 3.8749\n",
      "Time to train model \"g0-8\": 4.2679\n",
      "Time to train model \"g0-0\": 3.6448\n",
      "Time to train model \"g3-2\": 8.0275\n",
      "Time to train model \"g3-1\": 7.9199\n",
      "Time to train model \"g3-0\": 5.8587\n",
      "---------------------------------------------\n",
      "g4-0\tlr:0.2652\n",
      "\tSize: 512\tActivation: Sigmoid\tDropout: 0.03\n",
      "\tSize: 16\tActivation: ReLU\tDropout: 0.2\n",
      "---------------------------------------------\n",
      "---------------------------------------------\n",
      "g4-1\tlr:0.0734\n",
      "\tSize: 256\tActivation: LeakyReLU\tDropout: 0.15\n",
      "---------------------------------------------\n",
      "---------------------------------------------\n",
      "g4-2\tlr:0.2652\n",
      "\tSize: 512\tActivation: Sigmoid\tDropout: 0.03\n",
      "---------------------------------------------\n",
      "Best fitness for era 4: 0.9728\n",
      "Starting Era 5...\n",
      "Time to train model \"g0-7\": 8.5012\n",
      "Time to train model \"g0-1\": 5.5584\n",
      "Time to train model \"g0-6\": 8.4484\n",
      "Time to train model \"g0-5\": 4.4974\n",
      "Time to train model \"g4-2\": 8.2854\n",
      "Time to train model \"g4-1\": 5.8345\n",
      "Time to train model \"g4-0\": 8.1064\n",
      "Time to train model \"g3-2\": 8.0929\n",
      "Time to train model \"g3-1\": 7.8920\n",
      "Time to train model \"g3-0\": 5.7919\n",
      "---------------------------------------------\n",
      "g5-0\tlr:0.2652\n",
      "\tSize: 512\tActivation: LeakyReLU\tDropout: 0.13\n",
      "\tSize: 128\tActivation: Linear\tDropout: 0.37\n",
      "---------------------------------------------\n",
      "---------------------------------------------\n",
      "g5-1\tlr:0.2652\n",
      "\tSize: 256\tActivation: Sigmoid\tDropout: 0.13\n",
      "---------------------------------------------\n",
      "---------------------------------------------\n",
      "g5-2\tlr:0.2652\n",
      "\tSize: 512\tActivation: Linear\tDropout: 0.03\n",
      "\tSize: 1024\tActivation: Linear\tDropout: 0.07\n",
      "---------------------------------------------\n",
      "Best fitness for era 5: 0.9744\n",
      "Starting Era 6...\n",
      "Time to train model \"g0-7\": 8.4479\n",
      "Time to train model \"g5-2\": 16.0310\n",
      "Time to train model \"g5-1\": 5.5587\n",
      "Time to train model \"g5-0\": 9.6092\n",
      "Time to train model \"g4-2\": 8.1029\n",
      "Time to train model \"g4-1\": 5.7799\n",
      "Time to train model \"g4-0\": 8.0881\n",
      "Time to train model \"g3-2\": 8.2114\n",
      "Time to train model \"g3-1\": 7.8471\n",
      "Time to train model \"g3-0\": 5.8159\n",
      "Best fitness for era 6: 0.9732\n",
      "Starting Era 7...\n",
      "Time to train model \"g0-7\": 8.6005\n",
      "Time to train model \"g5-2\": 16.0203\n",
      "Time to train model \"g5-1\": 5.5081\n",
      "Time to train model \"g5-0\": 9.5959\n",
      "Time to train model \"g4-2\": 8.1150\n",
      "Time to train model \"g4-1\": 5.7904\n",
      "Time to train model \"g4-0\": 8.1865\n",
      "Time to train model \"g3-2\": 8.1015\n",
      "Time to train model \"g3-1\": 7.9150\n",
      "Time to train model \"g3-0\": 5.8369\n",
      "---------------------------------------------\n",
      "g7-0\tlr:0.0734\n",
      "\tSize: 256\tActivation: LeakyReLU\tDropout: 0\n",
      "---------------------------------------------\n",
      "---------------------------------------------\n",
      "g7-1\tlr:0.0734\n",
      "\tSize: 512\tActivation: LeakyReLU\tDropout: 0.03\n",
      "\tSize: 64\tActivation: Sigmoid\tDropout: 0.07\n",
      "---------------------------------------------\n",
      "---------------------------------------------\n",
      "g7-2\tlr:0.027\n",
      "\tSize: 256\tActivation: LeakyReLU\tDropout: 0.03\n",
      "\tSize: 64\tActivation: LeakyReLU\tDropout: 0.38\n",
      "---------------------------------------------\n",
      "Best fitness for era 7: 0.9734\n",
      "Starting Era 8...\n",
      "Time to train model \"g7-2\": 6.6491\n",
      "Time to train model \"g7-1\": 9.0716\n",
      "Time to train model \"g7-0\": 5.7758\n",
      "Time to train model \"g5-2\": 16.7111\n",
      "Time to train model \"g5-1\": 5.5490\n",
      "Time to train model \"g5-0\": 9.4859\n",
      "Time to train model \"g4-2\": 8.1370\n",
      "Time to train model \"g4-1\": 5.8161\n",
      "Time to train model \"g4-0\": 8.2590\n",
      "---------------------------------------------\n",
      "g8-0\tlr:0.0734\n",
      "\tSize: 256\tActivation: LeakyReLU\tDropout: 0.09\n",
      "\tSize: 128\tActivation: ReLU\tDropout: 0.09\n",
      "---------------------------------------------\n",
      "---------------------------------------------\n",
      "g8-1\tlr:0.0734\n",
      "\tSize: 256\tActivation: LeakyReLU\tDropout: 0.03\n",
      "---------------------------------------------\n",
      "---------------------------------------------\n",
      "g8-2\tlr:0.0734\n",
      "\tSize: 512\tActivation: LeakyReLU\tDropout: 0.03\n",
      "---------------------------------------------\n",
      "Best fitness for era 8: 0.9734\n",
      "Starting Era 9...\n",
      "Time to train model \"g8-2\": 8.6887\n",
      "Time to train model \"g8-1\": 5.8476\n",
      "Time to train model \"g8-0\": 6.7617\n",
      "Time to train model \"g7-2\": 6.7399\n",
      "Time to train model \"g7-1\": 9.1362\n",
      "Time to train model \"g7-0\": 5.6048\n",
      "Time to train model \"g5-2\": 16.1783\n",
      "Time to train model \"g5-1\": 5.5503\n",
      "Time to train model \"g5-0\": 9.6425\n",
      "---------------------------------------------\n",
      "g9-0\tlr:0.0734\n",
      "\tSize: 512\tActivation: LeakyReLU\tDropout: 0.03\n",
      "\tSize: 128\tActivation: Linear\tDropout: 0.37\n",
      "---------------------------------------------\n",
      "---------------------------------------------\n",
      "g9-1\tlr:0.2652\n",
      "\tSize: 512\tActivation: LeakyReLU\tDropout: 0.03\n",
      "---------------------------------------------\n",
      "---------------------------------------------\n",
      "g9-2\tlr:0.0734\n",
      "\tSize: 512\tActivation: LeakyReLU\tDropout: 0.13\n",
      "\tSize: 64\tActivation: Sigmoid\tDropout: 0.23\n",
      "---------------------------------------------\n",
      "Best fitness for era 9: 0.9740\n",
      "Starting Era 10...\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Time to train model \"g5-0\": 9.5093\n",
      "Time to train model \"g9-2\": 9.2941\n",
      "Time to train model \"g9-1\": 8.5241\n",
      "Time to train model \"g9-0\": 9.5736\n",
      "Time to train model \"g8-2\": 8.6359\n",
      "Time to train model \"g8-1\": 5.8188\n",
      "Time to train model \"g8-0\": 6.7320\n",
      "Time to train model \"g7-2\": 6.6860\n",
      "Time to train model \"g7-1\": 9.0949\n",
      "Time to train model \"g7-0\": 5.6355\n",
      "Best fitness for era 10: 0.9730\n",
      "Starting Era 11...\n",
      "Time to train model \"g5-0\": 9.6515\n",
      "Time to train model \"g9-2\": 9.1211\n"
     ]
    }
   ],
   "source": [
    "pool = Population()\n",
    "hist = pool.evolve(20)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Growth rate = 0.1, Shrink Rate = 0.1, Mutation Rate = 0.001\n",
    "\n",
    "Final best fitness after 20 eras: g13-1 0.9902\n",
    "Total training time: 1671.30\n",
    "    Tree #0\n",
    "    ---------------------------------------------\n",
    "    g13-1\tlr:0.1779\n",
    "        Fitness: 0.9901\tAccuracy: 0.9902\tLoss: 0.0593\n",
    "        Size: 512\tActivation: Tanh\tDropout: 0.01\n",
    "        Size: 32\tActivation: LeakyReLU\tDropout: 0.5\n",
    "    ---------------------------------------------"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "tensorflow",
   "language": "python",
   "name": "tensorflow"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
